{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression, Linear Classification and Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "#load dataset\n",
    "d=load_svmlight_file('a9a')\n",
    "d[1][np.where(d[1]==-1)]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=d[0].todense()\n",
    "y_train=d[1].reshape(len(d[1]),-1)\n",
    "#add x0\n",
    "m,n=np.shape(x_train)\n",
    "x_train=np.column_stack((np.ones(m),x_train))\n",
    "\n",
    "#add validset\n",
    "d=load_svmlight_file('a9a.t')\n",
    "d[1][np.where(d[1]==-1)]=0\n",
    "x_valid=d[0].todense()\n",
    "y_valid=d[1].reshape(len(d[1]),-1)\n",
    "p,q=np.shape(x_valid)\n",
    "x_valid=np.column_stack((np.ones(p),x_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "validation loss 4.05750348184\n",
      "validation accuracy 0.23622627602727106\n",
      "\n",
      "Epoch 10\n",
      "validation loss 1.23745542828\n",
      "validation accuracy 0.7640808304158221\n",
      "\n",
      "Epoch 20\n",
      "validation loss 0.479039183219\n",
      "validation accuracy 0.7660463116516184\n",
      "\n",
      "Epoch 30\n",
      "validation loss 0.402039975205\n",
      "validation accuracy 0.8164731896075179\n",
      "\n",
      "Epoch 40\n",
      "validation loss 0.385760383116\n",
      "validation accuracy 0.8253178551686015\n",
      "\n",
      "Epoch 50\n",
      "validation loss 0.369194612254\n",
      "validation accuracy 0.82943308150605\n",
      "\n",
      "Epoch 60\n",
      "validation loss 0.358729046151\n",
      "validation accuracy 0.8344696271727781\n",
      "\n",
      "Epoch 70\n",
      "validation loss 0.350639707007\n",
      "validation accuracy 0.8380320619126589\n",
      "\n",
      "Epoch 80\n",
      "validation loss 0.342518266202\n",
      "validation accuracy 0.8391376451077943\n",
      "\n",
      "Epoch 90\n",
      "validation loss 0.34269262662\n",
      "validation accuracy 0.8404274921687857\n",
      "\n",
      "Epoch 100\n",
      "validation loss 0.338499904135\n",
      "validation accuracy 0.8428843437135312\n",
      "\n",
      "Epoch 200\n",
      "validation loss 0.331641769149\n",
      "validation accuracy 0.8459554081444629\n",
      "\n",
      "Epoch 300\n",
      "validation loss 0.330611760713\n",
      "validation accuracy 0.8439899269086666\n",
      "\n",
      "Epoch 400\n",
      "validation loss 0.328083785387\n",
      "validation accuracy 0.847122412628217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gamma=0.9\n",
    "eta=0.1\n",
    "epochs=400\n",
    "epochstep=100\n",
    "epochset=[]\n",
    "l_NAG=[]\n",
    "grad=np.zeros((n+1,1))\n",
    "v=np.zeros((n+1,1))\n",
    "w=np.random.rand(n+1,1)\n",
    "batch_size=128\n",
    "\n",
    "def label(x):\n",
    "    if x<0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "for i in range (epochs+1):\n",
    "    #training\n",
    "    batch_bgn=np.random.randint(0,m-batch_size-1)\n",
    "    for j in range(batch_bgn,batch_bgn+batch_size):\n",
    "        h=1/(1+np.exp(x_train[j]*(gamma*v-w)))\n",
    "        grad=grad+((h-y_train[j])*x_train[j]).T\n",
    "    grad=grad/batch_size\n",
    "    v=gamma*v+eta*grad\n",
    "    w=w-v\n",
    "    if(i<100 and i%10==0) or (i%epochstep==0):\n",
    "        epochset.append(i)\n",
    "        #compute loss\n",
    "        loss=0.\n",
    "        correct=0\n",
    "        for k in range(p):\n",
    "            h=1/(1+np.exp(x_valid[k]*(gamma*v-w)))\n",
    "            loss+=(y_valid[k]*np.log(h)+(1-y_valid[k])*np.log(1-h))\n",
    "            if label(h)==y_valid[k]:\n",
    "                correct+=1\n",
    "        loss=np.asarray(-loss/p)\n",
    "        l_NAG.append(loss[0][0])\n",
    "        #print \n",
    "        print('Epoch',i)\n",
    "        print('validation loss',loss[0][0])\n",
    "        print('validation accuracy',correct/p)\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "validation loss 5.84413572011\n",
      "validation accuracy 0.23622627602727106\n",
      "\n",
      "Epoch 10\n",
      "validation loss 4.15999776952\n",
      "validation accuracy 0.23622627602727106\n",
      "\n",
      "Epoch 20\n",
      "validation loss 2.9464410294\n",
      "validation accuracy 0.23622627602727106\n",
      "\n",
      "Epoch 30\n",
      "validation loss 1.94274596217\n",
      "validation accuracy 0.23659480375898287\n",
      "\n",
      "Epoch 40\n",
      "validation loss 1.19089519936\n",
      "validation accuracy 0.2781155948651803\n",
      "\n",
      "Epoch 50\n",
      "validation loss 0.757372692594\n",
      "validation accuracy 0.5117621767704686\n",
      "\n",
      "Epoch 60\n",
      "validation loss 0.592837257165\n",
      "validation accuracy 0.6919722375775443\n",
      "\n",
      "Epoch 70\n",
      "validation loss 0.507714458833\n",
      "validation accuracy 0.7680117928874147\n",
      "\n",
      "Epoch 80\n",
      "validation loss 0.456741322162\n",
      "validation accuracy 0.7942386831275721\n",
      "\n",
      "Epoch 90\n",
      "validation loss 0.424222303201\n",
      "validation accuracy 0.807198575026104\n",
      "\n",
      "Epoch 100\n",
      "validation loss 0.39602110425\n",
      "validation accuracy 0.8175787728026535\n",
      "\n",
      "Epoch 200\n",
      "validation loss 0.32979515863\n",
      "validation accuracy 0.8484736809778269\n",
      "\n",
      "Epoch 300\n",
      "validation loss 0.326113599504\n",
      "validation accuracy 0.8495178428843437\n",
      "\n",
      "Epoch 400\n",
      "validation loss 0.333371339922\n",
      "validation accuracy 0.8415944966525398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gamma=0.9\n",
    "eta=0.01\n",
    "eps=1e-8\n",
    "epochset=[]\n",
    "l_RMSP=[]\n",
    "G=np.zeros((n+1,1))\n",
    "grad=np.zeros((n+1,1))\n",
    "w=np.random.rand(n+1,1)\n",
    "\n",
    "for i in range(epochs+1):\n",
    "    #training\n",
    "    batch_bgn=np.random.randint(m-batch_size-1)\n",
    "    for j in range(batch_bgn,batch_bgn+batch_size):\n",
    "        h=1/(1+np.exp(-x_train[j]*w))\n",
    "        grad+=((h-y_train[j])*x_train[j]).T\n",
    "    grad=grad/batch_size\n",
    "    G=gamma*G+np.multiply((1-gamma)*grad,grad)\n",
    "    w=w-np.multiply(eta/(np.sqrt(G+eps)),grad)\n",
    "    if(i<100 and i%10==0) or (i%epochstep==0):\n",
    "        epochset.append(i)\n",
    "        #compute loss\n",
    "        loss=0.\n",
    "        correct=0\n",
    "        for k in range(p):\n",
    "            h=1/(1+np.exp(-x_valid[k]*w))\n",
    "            loss+=(y_valid[k]*np.log(h)+(1-y_valid[k])*np.log(1-h))\n",
    "            if label(h)==y_valid[k]:\n",
    "                correct+=1\n",
    "        loss=np.asarray(-loss/p)\n",
    "        l_RMSP.append(loss[0][0])\n",
    "        #print \n",
    "        print('Epoch',i)\n",
    "        print('validation loss',loss[0][0])\n",
    "        print('validation accuracy',correct/p)\n",
    "        print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "validation loss 4.24743613913\n",
      "validation accuracy 0.23622627602727106\n",
      "\n",
      "Epoch 10\n",
      "validation loss 3.89158476937\n",
      "validation accuracy 0.23622627602727106\n",
      "\n",
      "Epoch 20\n",
      "validation loss 3.53721045077\n",
      "validation accuracy 0.23622627602727106\n",
      "\n",
      "Epoch 30\n",
      "validation loss 3.18739045721\n",
      "validation accuracy 0.23622627602727106\n",
      "\n",
      "Epoch 40\n",
      "validation loss 2.83853884736\n",
      "validation accuracy 0.23622627602727106\n",
      "\n",
      "Epoch 50\n",
      "validation loss 2.50188581\n",
      "validation accuracy 0.23622627602727106\n",
      "\n",
      "Epoch 60\n",
      "validation loss 2.19294222381\n",
      "validation accuracy 0.23659480375898287\n",
      "\n",
      "Epoch 70\n",
      "validation loss 1.9007002211\n",
      "validation accuracy 0.2387445488606351\n",
      "\n",
      "Epoch 80\n",
      "validation loss 1.64130059811\n",
      "validation accuracy 0.24525520545421042\n",
      "\n",
      "Epoch 90\n",
      "validation loss 1.42068760238\n",
      "validation accuracy 0.26171611080400464\n",
      "\n",
      "Epoch 100\n",
      "validation loss 1.23096860864\n",
      "validation accuracy 0.2873287881579756\n",
      "\n",
      "Epoch 200\n",
      "validation loss 0.573446482896\n",
      "validation accuracy 0.7116884712241263\n",
      "\n",
      "Epoch 300\n",
      "validation loss 0.484656401659\n",
      "validation accuracy 0.7816473189607518\n",
      "\n",
      "Epoch 400\n",
      "validation loss 0.448284646304\n",
      "validation accuracy 0.7971869049812665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gamma=0.999\n",
    "eps=1e-8\n",
    "delta_t=np.zeros((n+1,1))\n",
    "epochset=[]\n",
    "l_Ada=[]\n",
    "G=np.zeros((n+1,1))\n",
    "grad=np.zeros((n+1,1))\n",
    "w=np.random.rand(n+1,1)\n",
    "\n",
    "for i in range(epochs+1):\n",
    "    #training\n",
    "    batch_bgn=np.random.randint(0,m-batch_size-1)\n",
    "    for j in range(batch_bgn,batch_bgn+batch_size):\n",
    "        h=1/(1+np.exp(-x_train[j]*w))\n",
    "        grad+=((h-y_train[j])*x_train[j]).T\n",
    "    grad=grad/batch_size\n",
    "    G=gamma*G+np.multiply((1-gamma)*grad,grad)\n",
    "    delta_w=-np.multiply(np.sqrt(delta_t+eps),grad)/np.sqrt(G+eps)\n",
    "    delta_t=gamma*delta_t+np.multiply((1-gamma)*delta_w,delta_w)\n",
    "    w=w+delta_w\n",
    "    if(i<100 and i%10==0) or (i%epochstep==0):\n",
    "        epochset.append(i)\n",
    "        #compute loss\n",
    "        loss=0.\n",
    "        correct=0\n",
    "        for k in range(p):\n",
    "            h=1/(1+np.exp(-x_valid[k]*w))\n",
    "            loss+=(y_valid[k]*np.log(h)+(1-y_valid[k])*np.log(1-h))\n",
    "            if label(h)==y_valid[k]:\n",
    "                correct+=1\n",
    "        loss=np.asarray(-loss/p)\n",
    "        l_Ada.append(loss[0][0])\n",
    "        #print \n",
    "        print('Epoch',i)\n",
    "        print('validation loss',loss[0][0])\n",
    "        print('validation accuracy',correct/p)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "validation loss 3.87561910779\n",
      "validation accuracy 0.28001965481235797\n",
      "\n",
      "Epoch 10\n",
      "validation loss 3.64961446912\n",
      "validation accuracy 0.5296971930471102\n",
      "\n",
      "Epoch 20\n",
      "validation loss 3.87584931228\n",
      "validation accuracy 0.6319636385971378\n",
      "\n",
      "Epoch 30\n",
      "validation loss 3.86188786405\n",
      "validation accuracy 0.6630428106381672\n",
      "\n",
      "Epoch 40\n",
      "validation loss 3.78318926812\n",
      "validation accuracy 0.6626128616178367\n",
      "\n",
      "Epoch 50\n",
      "validation loss 3.7088770273\n",
      "validation accuracy 0.6615072784227013\n",
      "\n",
      "Epoch 60\n",
      "validation loss 3.6391502746\n",
      "validation accuracy 0.6546280940974142\n",
      "\n",
      "Epoch 70\n",
      "validation loss 3.58122796713\n",
      "validation accuracy 0.6547509366746515\n",
      "\n",
      "Epoch 80\n",
      "validation loss 3.5313533847\n",
      "validation accuracy 0.6653153983170567\n",
      "\n",
      "Epoch 90\n",
      "validation loss 3.48490096044\n",
      "validation accuracy 0.6756955960936061\n",
      "\n",
      "Epoch 100\n",
      "validation loss 3.44279273231\n",
      "validation accuracy 0.6863214790246299\n",
      "\n",
      "Epoch 200\n",
      "validation loss 3.14481095571\n",
      "validation accuracy 0.728579325594251\n",
      "\n",
      "Epoch 300\n",
      "validation loss 2.96691773781\n",
      "validation accuracy 0.7341686628585468\n",
      "\n",
      "Epoch 400\n",
      "validation loss 2.84439730445\n",
      "validation accuracy 0.7516737301148578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beta=0.88\n",
    "gamma=0.99\n",
    "eta=0.005\n",
    "eps=1e-8\n",
    "moment=np.random.rand(n+1,1)\n",
    "epochset=[]\n",
    "l_Adam=[]\n",
    "G=np.zeros((n+1,1))\n",
    "grad=np.zeros((n+1,1))\n",
    "w=np.random.rand(n+1,1)\n",
    "\n",
    "for i in range(epochs+1):\n",
    "    #training\n",
    "    batch_bgn=np.random.randint(0,m-batch_size-1)\n",
    "\n",
    "    for j in range(batch_bgn,batch_bgn+batch_size):\n",
    "        h=1/(1+np.exp(-x_train[j]*w))\n",
    "        grad+=((h-y_train[j])*x_train[j]).T\n",
    "    grad=grad/batch_size\n",
    "    moment=beta*moment+(1-beta)*grad\n",
    "    G=gamma*G+np.multiply((1-gamma)*grad,grad)\n",
    "    alpha=eta*np.sqrt(1-gamma)/(1-beta)\n",
    "    #if(i>0):\n",
    "     #   beta=beta/np.sqrt(i)\n",
    "      #  eta=eta/np.sqrt(i)\n",
    "    w=w-alpha*moment/np.sqrt(G+eps)\n",
    "    if(i<100 and i%10==0) or (i%epochstep==0):\n",
    "        epochset.append(i)\n",
    "        #compute loss\n",
    "        loss=0.\n",
    "        correct=0\n",
    "        for k in range(p):\n",
    "            h=1/(1+np.exp(-x_valid[k]*w))\n",
    "            loss+=(y_valid[k]*np.log(h)+(1-y_valid[k])*np.log(1-h))\n",
    "            if label(h)==y_valid[k]:\n",
    "                correct+=1\n",
    "        loss=np.asarray(-loss/p)\n",
    "        l_Adam.append(loss[0][0])\n",
    "        #print \n",
    "        print('Epoch',i)\n",
    "        print('validation loss',loss[0][0])\n",
    "        print('validation accuracy',correct/p)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw plot\n",
    "def draw_plot(epochset,l_NAG,l_RMSP,l_Ada,l_Adam):\n",
    "    plt.figure(figsize=(10,15))\n",
    "    plt.subplot(211)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(epochset,l_NAG,color='red',label='NAG',linewidth=1.5,linestyle='-')\n",
    "    plt.plot(epochset,l_RMSP,color='yellow',label='RMSProp',linewidth=1.5,linestyle='-')\n",
    "    plt.plot(epochset,l_Ada,color='blue',label='AdaDelta',linewidth=1.5,linestyle='-')\n",
    "    plt.plot(epochset,l_Adam,color='green',label='Adam',linewidth=1.5,linestyle='-')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "draw_plot(epochset,l_NAG,l_RMSP,l_Ada,l_Adam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
